# -*- coding: utf-8 -*-
"""RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XgHF7jBear0og38RSdF2Ll_S0yp2LqhM
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import requests
import math
import random
import os
from collections import deque
from keras.models import Sequential, load_model
from keras.layers import Dense, LeakyReLU, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.losses import MeanSquaredError

data_url = 'https://api.polygon.io/v2/aggs/ticker/NVDA/range/1/day/2022-01-01/2024-12-23?adjusted=true&sort=asc&limit=5000&apiKey=LJipqdFunGpcv8kKGLDs6VWhWrwKODAd'  # More Data
data = requests.get(data_url).json()
dataset = pd.DataFrame(data['results'])
dataset["datetime"] = pd.to_datetime(dataset["t"], unit="ms")
dataset['ticker'] = data['ticker']
X = dataset["c"].astype(float).tolist()

plt.figure(figsize=(12, 6))
plt.plot(dataset["datetime"], X, label="NVDA Closing Price", color='b')
plt.xlabel("Date")
plt.ylabel("Closing Price (USD)")
plt.title("NVDA Stock Closing Prices (2022-2024)")
plt.legend()
plt.grid(True)
plt.show()

train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]

def formatPrice(n):
    return ("-$" if n < 0 else "$") + "{0:.2f}".format(abs(n))

def sigmoid(x):
    return 1 / (1 + math.exp(-x))

def getState(data, t, n):
    d = t - n + 1
    block = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1]
    return np.array([sigmoid(block[i + 1] - block[i]) for i in range(n - 1)])

def calculate_metrics(trade_log):
    profits = np.array(trade_log)
    total_profit = np.sum(profits)
    win_rate = np.mean(profits > 0) if len(profits) > 0 else 0
    profit_factor = np.sum(profits[profits > 0]) / (abs(np.sum(profits[profits < 0])) + 1e-6)
    sharpe_ratio = np.mean(profits) / (np.std(profits) + 1e-6)
    max_drawdown = np.min(profits) if len(profits) > 0 else 0

    return {
        "Total Profit": formatPrice(total_profit),
        "Win Rate": f"{win_rate * 100:.2f}%",
        "Profit Factor": f"{profit_factor:.2f}",
        "Sharpe Ratio": f"{sharpe_ratio:.2f}",
        "Max Drawdown": formatPrice(max_drawdown)
    }

class Agent:
    def __init__(self, state_size, is_eval=False, model_name=""):
        self.state_size = state_size
        self.action_size = 3  # sit, buy, sell
        self.memory = deque(maxlen=15000)  # Increased Memory
        self.inventory = []
        self.model_name = model_name
        self.is_eval = is_eval

        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.99995 #Slower decay

        if is_eval:
            try:
                self.model = load_model(model_name)
                print("Model Loaded Successfully")
            except FileNotFoundError:
                print(f"Model file {model_name} not found. Train the model first.")
                raise
        else:
            self.model = self._model()

    def _model(self):
        model = Sequential([
            Dense(512, input_dim=self.state_size, kernel_regularizer='l2'), #Added Regularization
            BatchNormalization(),
            LeakyReLU(alpha=0.01),
            Dropout(0.25),
            Dense(256, kernel_regularizer='l2'),
            BatchNormalization(),
            LeakyReLU(alpha=0.01),
            Dropout(0.25),
            Dense(128, kernel_regularizer='l2'),
            BatchNormalization(),
            LeakyReLU(alpha=0.01),
            Dense(self.action_size, activation="linear")
        ])
        optimizer = Adam(learning_rate=0.0001, clipvalue=0.5) #Reduced learning rate
        model.compile(loss=MeanSquaredError(), optimizer=optimizer)
        return model

    def act(self, state):
        if not self.is_eval and random.random() <= self.epsilon:
            return random.randrange(self.action_size)
        return np.argmax(self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0])

    def expReplay(self, batch_size):
        mini_batch = random.sample(self.memory, min(len(self.memory), batch_size))
        states = np.array([m[0] for m in mini_batch])
        next_states = np.array([m[3] for m in mini_batch])
        targets = self.model.predict(states, verbose=0)
        next_q_values = self.model.predict(next_states, verbose=0)

        for i, (state, action, reward, next_state, done) in enumerate(mini_batch):
            targets[i][action] = reward if done else reward + self.gamma * np.amax(next_q_values[i])

        self.model.fit(states, targets, epochs=1, verbose=0)
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

window_size = 15
batch_size = 64
episode_count = 500 # Increased episodes
agent = Agent(window_size)

episode_profits = []

def plot_trading_actions(prices, states_buy, states_sell, episode_or_test):
    plt.figure(figsize=(10, 5))
    plt.plot(prices, label='Stock Price', color='black')
    plt.scatter(states_buy, [prices[i] for i in states_buy], marker='^', color='green', label='Buy', alpha=1)
    plt.scatter(states_sell, [prices[i] for i in states_sell], marker='v', color='red', label='Sell', alpha=1)
    plt.title(f'Trading Actions - {episode_or_test}')
    plt.legend()
    plt.show()

for e in range(episode_count + 1):
    print(f"Running Episode {e}/{episode_count}")
    state = getState(X_train, 0, window_size + 1)
    total_profit = 0
    agent.inventory = []
    states_buy, states_sell = [], []
    trade_log = []

    for t in range(len(X_train) - 1):
        action = agent.act(state)
        next_state = getState(X_train, t + 1, window_size + 1)
        reward = 0

        if action == 1:
            agent.inventory.append(X_train[t])
            states_buy.append(t)
        elif action == 2 and agent.inventory:
            bought_price = agent.inventory.pop(0)
            reward = X_train[t] - bought_price
            total_profit += reward
            trade_log.append(reward)
            states_sell.append(t)

        done = t == len(X_train) - 2
        agent.memory.append((state, action, reward, next_state, done))
        state = next_state

        if len(agent.memory) > batch_size and t % 10 == 0:
            agent.expReplay(batch_size)

    metrics = calculate_metrics(trade_log)
    print(f"Episode {e} Metrics: {metrics}")
    episode_profits.append(total_profit)
    plot_trading_actions(X_train, states_buy, states_sell, f"Episode {e}")

agent.model.save("best_model.h5")

plt.figure(figsize=(10, 5))
plt.plot(range(len(episode_profits)), episode_profits, marker='o', linestyle='-')
plt.title('Profit Trend Across Episodes')
plt.xlabel('Episode')
plt.ylabel('Total Profit')
plt.show()

test_agent = Agent(window_size, is_eval=True, model_name="best_model.h5")
test_state = getState(X_test, 0, window_size + 1)
test_total_profit = 0
test_agent.inventory = []
test_states_buy, test_states_sell = [], []
test_trade_log = []

for t in range(len(X_test) - 1):
    test_action = test_agent.act(test_state)
    print(f"Test Action: {test_action}")
    print(f"Test State: {test_state}")
    test_next_state = getState(X_test, t + 1, window_size + 1)
    test_reward = 0

    if test_action == 1:
        test_agent.inventory.append(X_test[t])
        test_states_buy.append(t)
    elif test_action == 2 and test_agent.inventory:
        bought_price = test_agent.inventory.pop(0)
        test_reward = X_test[t] - bought_price
        test_total_profit += test_reward
        test_trade_log.append(test_reward)
        test_states_sell.append(t)

    test_state = test_next_state

test_metrics = calculate_metrics(test_trade_log)
print(f"Test Metrics: {test_metrics}")
plot_trading_actions(X_test, test_states_buy, test_states_sell, "Test")